{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the [Machine Learning class](https://github.com/erachelson/MLclass) by [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en).\n",
    "\n",
    "This notebook is joint work by Remy Priem (primary author), Morgane Menz, Mostafa Meliani, Joseph Morlier and Emmanuel Rachelson.\n",
    "\n",
    "License: CC-BY-SA-NC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Gaussian Process Regression (practice session)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "A surrogate model is an engineering method used when an outcome of interest cannot be easily directly measured, so a model of the outcome is used instead. Most engineering design problems require experiments and/or simulations to evaluate design objective and constraint functions as function of the design variables. For example, in order to find the optimal airfoil shape for an aircraft wing, an engineer simulates the air flow around the wing for different shape variables (length, curvature, material, ...). For many real world problems, however, a single simulation can take many minutes, hours, or even days to complete. As a result, routine tasks such as design optimization, design space exploration, sensitivity analysis become near impossible since they require a great number of function evaluations.\n",
    "\n",
    "One way of alleviating this burden is by constructing approximation models, known as surrogate models, response surface models, *metamodels* or emulators, that mimic the behavior of the simulation model as closely as possible while being computationally cheaper to evaluate. Surrogate models are constructed using a data-driven approach. The exact, inner working of the simulation code is not assumed to be known (or even understood), solely the input-output behavior is important.\n",
    "\n",
    "There exist multiple ways to build an approximation of a function: Artificial Neural Networks, Radial Basis Functions, Support Vector Regressions... \n",
    "In all these metamodels (or surrogate models), a fundamental assumption is that the quantity of interest $y(x)$ can be written $(y(x) = \\hat{y}(x) + \\epsilon)$, where the residuals $\\epsilon$ are independently and identically distributed normal random variables, so that fitting the model $\\hat{y}(x)$ is performed by minimizing a measure over $\\epsilon$.\n",
    "\n",
    "For a practical example, take a look to this airfoil optimization tool: [http://mdolab.engin.umich.edu/webfoil](http://mdolab.engin.umich.edu/webfoil)\n",
    "\n",
    "It has been constructed by lot of offline computations and an excellent tool developed jointly by University of Michigan, Nasa, Onera and ISAE-SUPAERO called SMT: [https://github.com/SMTorg/SMT](https://github.com/SMTorg/SMT)\n",
    "\n",
    "The authors strongly encourage students to have a look to classical textbooks such as [1] (for the Machine learning community) or [2] (for the Aerospace engineering community).\n",
    "\n",
    "[1]  Carl Edward Rasmussen. Gaussian Processes in Machine Learning. In Advanced lectures on machine learning, pages 63–71. Springer, 2004. Available for download at [http://www.gaussianprocess.org/gpml](http://www.gaussianprocess.org/gpml).<br>\n",
    "[2]  Alexander Forrester, Andy Keane et al. Engineering design via surrogate modelling: a practical guide. John Wiley & Sons, 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell only to install smt in colab. If you run this notebook locally, just install smt from the terminal.\n",
    "!pip install smt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Gaussian Process Regression\n",
    "\n",
    "The main idea behind Kriging is that the 'errors' -or more accurately, the deviations from the base model- in the predicted values $\\hat{y}$, are not independent. Rather, we take the view that the errors are a systematic function of the locations of the samples.\n",
    "\n",
    "Please recall your [previous course on GPR](https://github.com/erachelson/MLclass/blob/master/5%20-%20Gaussian%20Processes/Gaussian%20Processes.ipynb).\n",
    "\n",
    "We wish to train a GPR model $\\mathcal{G} = \\lbrace \\mathbf{X}, \\mathbf{Y}, \\theta \\rbrace$ using the Squared Exponential function. The so-called SE Kernel is $k(x,x') =\\sigma_f^2\\exp\\left(-\\frac{(x-x')^2}{l^ 2}\\right)$.\n",
    "\n",
    "Given the input data $\\mathbf{y} = \\left[y_1,\\ldots,y_N\\right]$ and $\\mathbf{x} =  \\left[x_1,\\ldots,x_N\\right]$, and given a covariance kernel $k(x,x')$, a Gaussian Process regressor estimates the distribution of $y(x)$ as a Gaussian $\\mathcal{N}(\\mu,\\sigma)$ with: $\\mu = K_*(x)K^{-1} \\mathbf{y}$, $\\sigma(y)^2 = k(x,x) - K_*(x)K^{-1}K_*(x)^T$, where:\n",
    "\n",
    "$$K =\n",
    "\\begin{bmatrix}\n",
    "k(x_1,x_1) & \\ldots & k(x_1,x_N) \\\\\n",
    "\\vdots     & \\ddots & \\vdots \\\\\n",
    "k(x_1,x_N) & \\ldots & k(x_N,x_N)\n",
    "\\end{bmatrix}, \\\n",
    "K_*(x) = \\left[k(x_1, x), \\ldots, k(x_N,x)\\right]$$\n",
    "\n",
    "On the 1D example $f(x)= (x-3.5)\\sin{\\frac{(x-3.5)}{\\pi}}$ on $\\left [ 0, 25\\right ]$, we will have a naive approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fun(point):\n",
    "    return np.atleast_2d((point-3.5)*np.sin((point-3.5)/(np.pi)))\n",
    "\n",
    "X_plot = np.atleast_2d(np.linspace(0, 25, 10000)).T\n",
    "Y_plot = fun(X_plot)\n",
    "#y_gpr , y_std = gpr.predict(X_plot , return_std=True)\n",
    "\n",
    "lines = []\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "true_fun, = ax.plot(X_plot,Y_plot)\n",
    "lines.append(true_fun)\n",
    "ax.set_title('$x \\sin{x}$ function')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_function(point_1,point_2,theta,sig):\n",
    "    theta = np.asarray(theta)\n",
    "    size = point_1.shape\n",
    "    dist = (point_1-point_2)**2\n",
    "    k_12 = (sig**2) * np.exp(- np.sum(dist / theta**2))\n",
    "    return k_12\n",
    "\n",
    "def cov_matrix(points,theta,sig):\n",
    "    theta = np.asarray(theta)\n",
    "    size = points.shape\n",
    "    K = np.zeros((size[0],size[0]))\n",
    "    for i,point in enumerate(points):\n",
    "        K[:,i] = np.array([cov_function(point,point_1,theta,sig) for point_1 in points])\n",
    "    return K\n",
    "\n",
    "def cov_vect(point,points,theta,sig):\n",
    "    theta = np.asarray(theta)\n",
    "    size = points.shape\n",
    "    K = np.array([cov_function(point,point_1,theta,sig) for point_1 in points])\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1:\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Question 1.1**<br>\n",
    "Code the function that predicts the mean and the standard deviation of the Gaussian process.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code1.py\n",
    "### WRITE YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Question 1.2**<br>\n",
    "Test several values of $\\theta = [l, \\sigma_f]$ with  $l$ defined as the length-scale  (of oscillations) and  $\\sigma_f$ the amplitude. Look at the RMSE and R2 score that give an information on the accuracy of the model. RMSE must be close to 0 and R2 must be close to 1. For a reminder about these metrics, you can refer to https://en.wikipedia.org/wiki/Root-mean-square_deviation and https://en.wikipedia.org/wiki/Coefficient_of_determination.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "x_test = np.atleast_2d(np.linspace(0,25,100)).T\n",
    "y_test = fun(x_test)\n",
    "\n",
    "x_data = np.atleast_2d([0,11,20,1,5,15,12,3,17]).T\n",
    "y_data = fun(x_data)\n",
    "\n",
    "\n",
    "X_plot = np.atleast_2d(np.linspace(0,25,1000)).T\n",
    "Y_plot = fun(X_plot)\n",
    "\n",
    "lines = []\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "true_fun, = ax.plot(X_plot,Y_plot)\n",
    "data, = ax.plot(x_data,y_data,linestyle='',marker='o')\n",
    "lines.append(true_fun)\n",
    "lines.append(data)\n",
    "ax.set_title('$x \\sin{x}$ function')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend(lines,['True function','Data'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = [3]\n",
    "sig = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code2.py\n",
    "### WRITE YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RMSE = mean_squared_error(y_test,y_pred[:,0])\n",
    "R2 = r2_score(y_test,y_pred[:,0])\n",
    "print('RMSE = %.5f' %(RMSE)) \n",
    "print('R2 = %.5f' %(R2))\n",
    "\n",
    "Y_GP_plot = np.array([myGPpredict(x_t,x_data,y_data,K_inv,theta,sig) \\\n",
    "                   for x_t in X_plot])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "true_fun, = ax.plot(X_plot,Y_plot)\n",
    "data, = ax.plot(x_data,y_data,linestyle='',marker='o')\n",
    "gp, = ax.plot(X_plot,Y_GP_plot[:,0],linestyle='--',color='g')\n",
    "un_gp = ax.fill_between(X_plot.T[0],Y_GP_plot[:,0]+3*Y_GP_plot[:,1],Y_GP_plot[:,0]-3*Y_GP_plot[:,1],alpha=0.3,color='g')\n",
    "lines = [true_fun,data,gp,un_gp]\n",
    "ax.set_title('$x \\sin{x}$ function')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend(lines,['True function','Data','GPR prediction','99 % confidence'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Question 1.3**<br>\n",
    "Conclude: Is there a way to find a $\\theta_{optimal} = \\theta^*$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-danger\"><a href=\"#answers1-3\" data-toggle=\"collapse\">\n",
    "    \n",
    "**Correction (click to unhide):**</a><br>\n",
    "<div id=\"answers1-3\" class=\"collapse\">\n",
    "TODO\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Hyperparameters Optimization\n",
    "\n",
    "To train the model, the negative log marginal likelihood with respect to the hyper-parameters is minimized:\n",
    "\n",
    "$$-\\text{log}\\, p(\\mathbf{Y} \\mid \\mathbf{X}, \\theta) = \\frac{N}{2} \\log(\\frac{\\mathbf{Y}^\\top\\mathbf{K}^{-1}\\mathbf{Y}}{N}) + \\frac{1}{2}\\log\\mid\\mathbf{K}\\mid + \\,c,$$\n",
    "\n",
    "where $c$ is a constant and the matrix $\\mathbf{K}$ is a function of the hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2:\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Question 2.1**<br>\n",
    "Plot a 1D graph of the Marginal Likelihood function for $l \\in [10^{-3},1.25]$ at $\\sigma_f$ fixed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code3.py\n",
    "### WRITE YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.linspace(1e-3,25)\n",
    "\n",
    "like = np.array([likelihood(x_data,y_data,th,sig) for th in theta])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "line = ax.plot(theta,like)\n",
    "ax.set_xlabel('Theta')\n",
    "ax.set_ylabel('Log Likelihood')\n",
    "ax.set_title('Log Likelihood regarding theta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Question 2.2**<br>\n",
    "Optimize the Marginal Likelihood function to find $l^*$.<br>Watch out: some optimizers are gradient-based and only converge to a local optimum (try different initializations).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize \n",
    "\n",
    "like_obj = lambda theta : likelihood(x_data,y_data,theta,sig)\n",
    "\n",
    "\n",
    "# Multistart to remove bad optimization results\n",
    "theta_start = np.linspace(1e-6,25,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code4.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_success = opt_all[[opt_i['success'] for opt_i in opt_all]]\n",
    "obj_success = np.array([opt_i['fun'] for opt_i in opt_success])\n",
    "ind_min = np.argmin(obj_success)\n",
    "\n",
    "opt = opt_success[ind_min]\n",
    "theta_et = opt['x']\n",
    "\n",
    "print('Optimization results', opt)\n",
    "print('')\n",
    "print('Best theta is %.5f' %(theta_et))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Optimization\n",
    "\n",
    "Now we would like to minimize a black box function subject to boundary constraints. This function is very expensive to evaluate and only provides a scalar output (meaning that you don't have access to gradients or higher order derivatives). We will use Gaussian Process to solve this optimization problem. \n",
    "\n",
    "Right now you can use directly smt which includes all these operations in a single procedure. Here, the data are the points $x_{data} = \\left [0, 7, 25 \\right ]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 3\n",
    "\n",
    "The `fun` function from section I will play the role of our \"expensive to evaluate function\".\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Question 3.1**<br>\n",
    "Build the GP model with a square exponential kernel with surrogate modelling toolbox (SMT) knowing $(x_{data}, y_{data})$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smt.surrogate_models import KRG\n",
    "\n",
    "x_data = np.atleast_2d([0,7,25]).T\n",
    "y_data = fun(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code5.py\n",
    "### WRITE YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_GP_plot = gpr.predict_values(X_plot)\n",
    "sig_GP_plot = gpr.predict_variances(X_plot)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "true_fun, = ax.plot(X_plot,Y_plot)\n",
    "data, = ax.plot(x_data,y_data,linestyle='',marker='o')\n",
    "gp, = ax.plot(X_plot,Y_GP_plot,linestyle='--',color='g')\n",
    "sig_plus = Y_GP_plot+3*np.atleast_2d(sig_GP_plot)\n",
    "sig_moins = Y_GP_plot-3*np.atleast_2d(sig_GP_plot)\n",
    "un_gp = ax.fill_between(X_plot.T[0],sig_plus.T[0],sig_moins.T[0],alpha=0.3,color='g')\n",
    "lines = [true_fun,data,gp,un_gp]\n",
    "ax.set_title('$x \\sin{x}$ function')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend(lines,['True function','Data','GPR prediction','99 % confidence'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 4\n",
    "\n",
    "Suppose you are confronted to an optimization problem where each evaluation of the objective function is very expensive: let's say 10 hours per evaluation. You want to solve this problem under *computational budget contraints*.\n",
    "\n",
    "Your challenge is: Minimize the $x \\mapsto x \\sin{(x)}$ function given a total computational budget of 10 points with the initial points $(x_{data},y_{data})$.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Question 4.1**<br>\n",
    "Give at least 2 methods to optimize the true function thanks to Gaussian Processes. You don't have to code it explicitly, just give the main idea.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-danger\"><a href=\"#answers4-1\" data-toggle=\"collapse\">\n",
    "    \n",
    "**Correction (click to unhide):**</a><br>\n",
    "<div id=\"answers4-1\" class=\"collapse\">\n",
    "TODO\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 5 : Bayesian Optimization\n",
    "\n",
    "Bayesian optimization is defined by Jonas Mockus in [3] as an optimization technique based upon the minimization of the expected deviation from the extremum of the studied function. \n",
    "\n",
    "[3]  J. Močkus.  On bayesian methods for seeking the extremum, pages 400–404. Springer Berlin Heidelberg, Berlin, Heidelberg, 1975.\n",
    "\n",
    "The objective function is treated as a black-box function. A Bayesian strategy sees the objective as a random function and places a prior over it. The prior captures our beliefs about the behavior of the function. After gathering the function evaluations, which are treated as data, the prior is updated to form the posterior distribution over the objective function. The posterior distribution, in turn, is used to construct an acquisition function (often also referred to as infill sampling criterion) that determines what the next query point should be.\n",
    "\n",
    "One of the earliest bodies of work on Bayesian optimisation is [4,5]. Kushner used [Wiener processes](https://en.wikipedia.org/wiki/Stochastic_process#Wiener_process) (Brownian motion processes) for one-dimensional problems. Kushner’s decision model was based on maximizing the probability of improvement, and included a parameter that controlled the trade-off between ‘more global’ and ‘more local’ optimization, in the same spirit as the Exploration/Exploitation trade-off.\n",
    "\n",
    "[4] Harold J Kushner. A Versatile Stochastic Model of a Function of Unknown and Time-Varying Form. vol. 5, pages 150–167, 08 1962.<br>\n",
    "[5] Harold J Kushner. A New Method of Locating the Maximum Point of an Arbitrary Multipeak Curve in the Presence of Noise. vol. 86, 01 1964.\n",
    "\n",
    "Meanwhile, in the former Soviet Union, Mockus and colleagues developed a multidimensional Bayesian optimization method using linear combinations of Wiener fields, some of which was published in English in [3]. This paper also describes an acquisition function that is based on myopic expected improvement of the posterior, which has been widely adopted in Bayesian optimization as the Expected Improvement function.\n",
    "\n",
    "In 1998, Jones used Gaussian processes together with the expected improvement function to successfully perform derivative-free optimization and experimental design through an algorithm called  Efficient  Global  Optimization, or EGO [6].\n",
    "\n",
    "[6] Donald R. Jones, Matthias Schonlau and William J. Welch. Efficient Global Optimization of Expensive Black-Box Functions. J. of Global Optimization, vol. 13, no. 4, pages 455–492, 1998.\n",
    "\n",
    "## Efficient Global Optimization\n",
    "\n",
    "In what follows, we describe the Efficient Global Optimization (EGO) algorithm, as published in |6].\n",
    "\n",
    "Let $F$ be an expensive black-box function to be minimized. We sample $F$ at the different locations  $X = \\{x_1, x_2,\\ldots,x_N\\}$ yielding the responses $Y = \\{y_1, y_2,\\ldots,y_N\\}$. We denote $D=(X,Y)$ the corresponding design of experiment with $Card(D)=N$. We build a Kriging model (also called Gaussian process) with a mean function $\\mu_D$ and a variance function $\\sigma^{2}_D$. For each point $x$ in the design space, the model prediction $y_x^D$ follows a gaussian distribution. $y_x^D \\sim \\mathcal{N}(\\mu_D(x),\\sigma^{2}_D(x))$ \n",
    "\n",
    "The next step is to compute the criterion EI. To do this, let us write:\n",
    "\n",
    "$$f_{min} = \\min \\{y_1, y_2,\\ldots,y_n\\} = min(Y).$$\n",
    "\n",
    "The Expected Improvement funtion (EI) can be expressed as:\n",
    "\n",
    "$$E[I(x)] = E[\\max(f_{min}-y_x^D, 0)],$$\n",
    "\n",
    "By rewriting the right-hand side of EI's expression as an integral, and applying some tedious integration by parts, one can express the expected improvement in closed form:\n",
    "\n",
    "\\begin{equation}\n",
    "E[I(x)] = (f_{min} - \\mu_D(x))\\Phi\\left(\\frac{f_{min} - \\mu_D(x)}{\\sigma_D(x)}\\right) + \\sigma_D(x) \\phi\\left(\\frac{f_{min} - \\mu_D(x)}{\\sigma_D(x)}\\right)\n",
    "\\label{eq:EI_simp}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\Phi(\\cdot)$ and $\\phi(\\cdot)$ are respectively the cumulative and probability density functions of $\\mathcal{N}(0,1)$.\n",
    "\n",
    "Next, we determine our next sampling point as :\n",
    "\\begin{align}\n",
    "x_{n+1} = \\arg \\max_{x} \\left(E[I(x)]\\right)\n",
    "\\end{align}\n",
    "\n",
    "We then test the response $y_{n+1}$ of our black-box function $F$ at $x_{n+1}$, rebuild the model taking into account the new information gained, and research the point of maximum expected improvement again.\n",
    "\n",
    "We summarize here the EGO algorithm:\n",
    "\n",
    "EGO(F, $n_{iter}$ \\# Find the best minimum of $\\operatorname{F}$ in $n_{iter}$ iterations  \n",
    "For ($i=0:n_{iter}$)  \n",
    "\n",
    "* $mod = {model}(X, Y)$  \\# surrogate model based on sample vectors $X$ and $Y$  \n",
    "* $f_{min} = \\min Y$  \n",
    "* $x_{i+1} = \\arg \\max {EI}(mod, f_{min})$ \\# choose $x$ that maximizes EI  \n",
    "* $y_{i+1} = {F}(x_{i+1})$ \\# Probe the function at most promising point $x_{i+1}$  \n",
    "* $X = [X,x_{i+1}]$  \n",
    "* $Y = [Y,y_{i+1}]$   \n",
    "* $i = i+1$  \n",
    "\n",
    "$f_{min} = \\min Y$  \n",
    "Return : $f_{min}$ \\# This is the best known solution after $n_{iter}$ iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Question 5.1**<br>\n",
    "Implement the Expected Improvement function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code6.py\n",
    "### WRITE YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_GP_plot = gpr.predict_values(X_plot)\n",
    "sig_GP_Plot = gpr.predict_variances(X_plot)\n",
    "Y_EI_plot = EI(gpr,X_plot,np.min(y_data))\n",
    "print(Y_EI_plot.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "true_fun, = ax.plot(X_plot,Y_plot)\n",
    "data, = ax.plot(x_data,y_data,linestyle='',marker='o')\n",
    "gp, = ax.plot(X_plot,Y_GP_plot,linestyle='--',color='g')\n",
    "sig_plus = Y_GP_plot+3*np.atleast_2d(sig_GP_Plot)\n",
    "sig_moins = Y_GP_plot-3*np.atleast_2d(sig_GP_Plot)\n",
    "un_gp = ax.fill_between(X_plot.T[0],sig_plus.T[0],sig_moins.T[0],alpha=0.3,color='g')\n",
    "ax1 = ax.twinx()\n",
    "ei, = ax1.plot(X_plot,Y_EI_plot,color='red')\n",
    "lines = [true_fun,data,gp,un_gp,ei]\n",
    "ax.set_title('$x \\sin{x}$ function')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax1.set_ylabel('ei')\n",
    "fig.legend(lines,['True function','Data','GPR prediction','99 % confidence','Expected Improvement'],loc=[0.13,0.64])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Question 5.2**<br>\n",
    "Complete the code of the EGO method and compare it to other infill criteria.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SBO(GP,point):\n",
    "    res = GP.predict(point)\n",
    "    return res\n",
    "\n",
    "def UCB(GP,point):\n",
    "    pred = GP.predict(point,return_std=True)\n",
    "    return pred[0]-3.*pred[1]\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "x_data = np.atleast_2d([0,7,25]).T\n",
    "y_data = fun(x_data)\n",
    "\n",
    "n_iter = 17\n",
    "\n",
    "gpr = KRG(theta0=[1e-2]*x_data.shape[1],print_prediction = False,print_global=False)\n",
    "gpr.set_training_values(x_data,y_data)\n",
    "\n",
    "gpr.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code7.py\n",
    "### WRITE YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Results : X = %s, Y = %s' %(x_opt,y_opt))\n",
    "\n",
    "Y_GP_plot = gpr.predict_values(X_plot)\n",
    "sig_GP_Plot = gpr.predict_variances(X_plot)\n",
    "Y_EI_plot = EI(gpr,X_plot,np.min(y_data))\n",
    "print(np.max(Y_EI_plot))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "true_fun, = ax.plot(X_plot,Y_plot)\n",
    "data, = ax.plot(x_data,y_data,linestyle='',marker='o')\n",
    "gp, = ax.plot(X_plot,Y_GP_plot,linestyle='--',color='g')\n",
    "sig_plus = Y_GP_plot+3*np.atleast_2d(sig_GP_Plot)\n",
    "sig_moins = Y_GP_plot-3*np.atleast_2d(sig_GP_Plot)\n",
    "un_gp = ax.fill_between(X_plot.T[0],sig_plus.T[0],sig_moins.T[0],alpha=0.3,color='g')\n",
    "ei, = ax.plot(X_plot,Y_EI_plot,color='red')\n",
    "lines = [true_fun,data,gp,un_gp,ei]\n",
    "ax.set_title('$x \\sin{x}$ function')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend(lines,['True function','Data','GPR prediction','99 % confidence', 'Expected Improvement'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super Efficient Global Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we describe the Super Efficient Global Optimization (SEGO) algorithm as published in [].\n",
    "SEGO is a direct extension of EGO for constrained optimization problems.\n",
    "\n",
    "Let the following constrained optimization problem:\n",
    "\\begin{equation}\n",
    "    x^* = \\underset{x\\in \\Omega}{argmin} ~~ f(x)  ~~ \\mbox{ such that } ~~ g(x)\\geq 0  \\mbox{ and} ~~ h(x)=0\n",
    "\\end{equation}\n",
    "where the constraints are defined by :\n",
    "\n",
    "$g: \\mathbb{R}^d \\to \\mathbb{R}^m$ ($m$ inequality constraints)\n",
    "\n",
    "$h: \\mathbb{R}^d \\to \\mathbb{R}^p$ ($p$ equality constraints)\n",
    "\n",
    "\n",
    "The Constrained Bayesian optimization (CBO) algorithm is quite similar as the one of the unconstrained BO approach except that the optimization sub-problem solved to enrich the DoE takes into account the constraints. The associated sub-problem can take two forms: it can be unconstrained and tries to optimize an adapted function which gathers the constraints and the classical criterion [7]; or it can be constrained and optimizes one of the previous acquisition functions with some feasibility criteria associated to the constraints $g$ and $h$ [10]. Here the focus is made on constrained optimization sub-problem methods. The optimization sub-problem is of the form\n",
    "\\begin{equation}\n",
    "    x_{next} = \\underset{x \\in \\Omega}{argmax} ~ \\alpha (x)  ~~ \\text{with} ~~  x \\in \\Omega_h \\cap \\Omega_g\n",
    "    \\label{Constrained enrichment optimization sub-problem}\n",
    "\\end{equation}\n",
    "where $\\alpha$ is the acquisition function (the expected improvement for example) $\\Omega_h$ and $\\Omega_g$ are respectively the feasible domains defined by the two feasibility criteria: $\\alpha_h: \\mathbb{R}^d \\to \\mathbb{R}^p$ and $\\alpha_g: \\mathbb{R}^d \\to \\mathbb{R}^m$. To construct the feasibility criteria, the approaches named Super Efficient Global Optimization (SEGO) [11] and the Super Efficient Global Optimization coupled with Mixture Of Experts (SEGOMOE) [8,9,10] use the posterior means of the GPs that modelize the constraints as feasibility criterion: $\\alpha_h= \\hat{\\mu}_h$ and $\\alpha_g = \\hat{\\mu}_g$. The feasible domains are $\\Omega_h = \\{ x, \\alpha_h(x)=0 \\}$ and $\\Omega_g = \\{ x, \\alpha_g(x) \\geq 0 \\}$. %As things progress, the feasible domain $\\Omega_h \\cap \\Omega_g$ will become clearer. \n",
    "\n",
    "\n",
    "[7] Hern ́andez-Lobato,  J.M.,  Gelbart,  M.A.,  Adams,  R.P.,  Hoffman,  M.W.,  Ghahramani,  Z.:  A  generalframework for constrained bayesian optimization using information-based search (2016)\n",
    "\n",
    "[8] Bartoli, N., Bouhlel, M.A., Kurek, I., Lafage, R., Lefebvre, T., Morlier, J., Priem, R., Stilz, V., Regis,R.:  Improvement  of  efficient  global  optimization  with  application  to  aircraft  wing  design.  In:  17thAIAA/ISSMO Multidisciplinary analysis and optimization conference. p. 4001 (2016)\n",
    " \n",
    "[9] Bartoli, N., Lefebvre, T., Dubreuil, S., Olivanti, R., Bons, N., Martins, J.R., Bouhlel, M.A., Morlier,J.:  An  adaptive  optimization  strategy  based  on  mixture  of  experts  for  wing  aerodynamic  design  op-timization.  In:  18th  AIAA/ISSMO  Multidisciplinary  Analysis  and  Optimization  Conference.  p.  4433(2017)\n",
    "  \n",
    "[10] Bartoli,  N.,  Lefebvre,  T.,  Dubreuil,  S.,  Olivanti,  R.,  Priem,  R.,  Bons,  N.,  Martins,  J.R.,  Morlier,  J.:Adaptive modeling strategy for constrained global optimization with application to aerodynamic wingdesign. Aerospace Science and technology90, 85–102 (2019)\n",
    "\n",
    "[11] Sasena, M.J.: Flexibility and efficiency enhancements for constrained global design optimization withkriging approximations. Ph.D. thesis, Citeseer (2002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we will consider the same function as before as objective function: $f(x)= (x-3.5)\\sin{\\frac{(x-3.5)}{\\pi}}$ on $\\left [ 0, 25\\right ]$.\n",
    "\n",
    "An inequality constraint is added: $x < 11$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Question 5.3**<br>\n",
    "Complete the code the constraint function $g$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code8.py\n",
    "### WRITE YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.atleast_2d([0,7,25]).T\n",
    "y_data = fun(x_data)\n",
    "y_data_con = g(x_data)\n",
    "    \n",
    "n_iter = 17\n",
    "\n",
    "# construct the grp for the objective\n",
    "gpr = KRG(theta0=[1e-2]*x_data.shape[1],print_prediction = False,print_global=False)\n",
    "gpr.set_training_values(x_data,y_data)\n",
    "\n",
    "gpr.train()\n",
    "\n",
    "# construct the grp for the constraint\n",
    "gpr_con = KRG(theta0=[1e-2]*x_data.shape[1],print_prediction = False,print_global=False)\n",
    "gpr_con.set_training_values(x_data,y_data_con)\n",
    "\n",
    "gpr_con.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Question 5.4**<br>\n",
    "Complete the code of the SEGO method and compare it to other infill criteria.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code9.py\n",
    "### WRITE YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results : X = %s, Y = %s' %(x_opt,y_opt))\n",
    "\n",
    "Y_GP_plot = gpr.predict_values(X_plot)\n",
    "sig_GP_Plot = gpr.predict_variances(X_plot)\n",
    "Y_con_GP_plot = gpr_con.predict_values(X_plot)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "true_fun, = ax.plot(X_plot,Y_plot)\n",
    "data, = ax.plot(x_data,y_data,linestyle='',marker='o')\n",
    "gp, = ax.plot(X_plot,Y_GP_plot,linestyle='--',color='g')\n",
    "sig_plus = Y_GP_plot+3*np.atleast_2d(sig_GP_Plot)\n",
    "sig_moins = Y_GP_plot-3*np.atleast_2d(sig_GP_Plot)\n",
    "un_gp = ax.fill_between(X_plot.T[0],sig_plus.T[0],sig_moins.T[0],alpha=0.3,color='g')\n",
    "gp_con, = ax.plot(X_plot,Y_con_GP_plot,linestyle='--',color='pink')\n",
    "\n",
    "index_lim = 440\n",
    "print(\"X_plot[index_lim]=\",X_plot[index_lim])\n",
    "\n",
    "unfeasible_domain = ax.fill_between(X_plot.T[0,index_lim:], 0, 1,\n",
    "                color='pink', alpha=0.3, transform=ax.get_xaxis_transform())\n",
    "\n",
    "lines = [true_fun,data,gp,un_gp,gp_con,unfeasible_domain]\n",
    "ax.set_title('$x \\sin{x}$ function')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend(lines,['True function','Data','GPR prediction','99 % confidence','GPR prediction for the constraint',\"unfeasible domain\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap: Play with a web app from Rasmussen\n",
    "\n",
    "[http://www.tmpl.fi/gp/](http://www.tmpl.fi/gp/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
